{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "292409e6",
   "metadata": {},
   "source": [
    "# Project 4 – Predicting a Continuous Target with Regression (Titanic)\n",
    "\n",
    "- **Author:** Deb St. Cyr\n",
    "- **Date:** November 14, 2025\n",
    "\n",
    "### **Project Overview**\n",
    "In this project, I explore how different features and modeling techniques impact the accuracy of predicting passenger fare. Unlike earlier modules that used classification to predict whether a passenger survived, this project shifts to **regression**, where the goal is to estimate a numeric value.\n",
    "\n",
    "I begin by preparing the Titanic dataset, handling missing values, engineering new features, and converting categorical variables into numeric form. Then I build multiple linear regression models using different feature combinations to understand how well each set explains variation in fare.\n",
    "\n",
    "After identifying the best-performing feature set, I extend the analysis using alternative regression techniques—including **Ridge**, **Elastic Net**, and **Polynomial Regression**—to compare how model complexity and regularization affect predictive performance.\n",
    "\n",
    "This notebook demonstrates a complete regression workflow:\n",
    "- Data cleaning and preprocessing  \n",
    "- Feature engineering  \n",
    "- Multiple model training and evaluation  \n",
    "- Interpretation of model behavior  \n",
    "- Comparison of linear and nonlinear regression methods  \n",
    "\n",
    "The goal is to understand which features are most useful for fare prediction and how different regression approaches influence accuracy, stability, and generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86a724d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, ElasticNet\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eef1b1f",
   "metadata": {},
   "source": [
    "## 1. Import and Inspect the Data\n",
    "Load the Titanic data from seaborn and confirm dataset structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "512eb306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked</th>\n",
       "      <th>class</th>\n",
       "      <th>who</th>\n",
       "      <th>adult_male</th>\n",
       "      <th>deck</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alive</th>\n",
       "      <th>alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived  pclass     sex   age  sibsp  parch     fare embarked  class  \\\n",
       "0         0       3    male  22.0      1      0   7.2500        S  Third   \n",
       "1         1       1  female  38.0      1      0  71.2833        C  First   \n",
       "2         1       3  female  26.0      0      0   7.9250        S  Third   \n",
       "3         1       1  female  35.0      1      0  53.1000        S  First   \n",
       "4         0       3    male  35.0      0      0   8.0500        S  Third   \n",
       "\n",
       "     who  adult_male deck  embark_town alive  alone  \n",
       "0    man        True  NaN  Southampton    no  False  \n",
       "1  woman       False    C    Cherbourg   yes  False  \n",
       "2  woman       False  NaN  Southampton   yes   True  \n",
       "3  woman       False    C  Southampton   yes  False  \n",
       "4    man        True  NaN  Southampton    no   True  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic = sns.load_dataset(\"titanic\")\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f04594",
   "metadata": {},
   "source": [
    "## 2. Data Exploration and Preparation\n",
    "\n",
    "Steps:\n",
    "- Impute missing age values (median)\n",
    "- Drop rows missing fare\n",
    "- Create `family_size`\n",
    "- Optional categorical encodings (sex, embarked)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a5e6c42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare',\n",
       "       'embarked', 'class', 'who', 'adult_male', 'deck', 'embark_town',\n",
       "       'alive', 'alone'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4a6310f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>family_size</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>7.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  family_size  pclass  sex     fare\n",
       "0  22.0            2       3    1   7.2500\n",
       "1  38.0            2       1    0  71.2833\n",
       "2  26.0            1       3    0   7.9250\n",
       "3  35.0            2       1    0  53.1000\n",
       "4  35.0            1       3    1   8.0500"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Impute age\n",
    "titanic['age'] = titanic['age'].fillna(titanic['age'].median())\n",
    "\n",
    "# Remove rows with missing fare\n",
    "titanic = titanic.dropna(subset=['fare'])\n",
    "\n",
    "# Feature engineering\n",
    "titanic['family_size'] = titanic['sibsp'] + titanic['parch'] + 1\n",
    "\n",
    "# Convert sex to numeric safely using category codes\n",
    "titanic['sex'] = titanic['sex'].astype('category').cat.codes\n",
    "\n",
    "# Keep only needed columns to avoid NaN in deck/embark_town/etc.\n",
    "titanic = titanic[['age', 'family_size', 'pclass', 'sex', 'fare']].dropna()\n",
    "\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b052b3f",
   "metadata": {},
   "source": [
    "## 3. Feature Selection and Justification\n",
    "\n",
    "We evaluate four cases:\n",
    "\n",
    "1. Case 1: age  \n",
    "2. Case 2: family_size  \n",
    "3. Case 3: age + family_size  \n",
    "4. Case 4: My choice (to be defined)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1644064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 1\n",
    "X1 = titanic[['age']]\n",
    "y1 = titanic['fare']\n",
    "\n",
    "# Case 2\n",
    "X2 = titanic[['family_size']]\n",
    "y2 = titanic['fare']\n",
    "\n",
    "# Case 3\n",
    "X3 = titanic[['age', 'family_size']]\n",
    "y3 = titanic['fare']\n",
    "\n",
    "# Case 4 – choose your own\n",
    "# For now: pclass + sex (common fare predictors)\n",
    "X4 = titanic[['pclass', 'sex']]\n",
    "y4 = titanic['fare']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4384a4",
   "metadata": {},
   "source": [
    "### Section 3 Reflection – Feature Selection\n",
    "\n",
    "1. **Why might these features affect a passenger’s fare?**\n",
    "\n",
    "   - **Age:** Age can sometimes relate to fare if ticket pricing differed for children vs. adults, or if traveling adults tended to book higher or lower class accommodations. Although the effect is small, age could still provide a slight relationship to fare.\n",
    "   - **Family Size:** Larger groups may have purchased tickets together or traveled in lower classes to reduce cost, which could influence fare patterns.\n",
    "   - **Pclass:** Class is strongly related to fare because 1st class tickets were significantly more expensive than 2nd and 3rd class. This is one of the most important predictors.\n",
    "   - **Sex:** Sex may relate indirectly to fare because men and women tended to occupy different cabins or travel in different groups, which might correlate with class or ticket type.\n",
    "\n",
    "2. **List all available features in the original Titanic dataset.**\n",
    "\n",
    "   The full dataset includes:  \n",
    "   `survived`, `pclass`, `sex`, `age`, `sibsp`, `parch`, `fare`, `class`,  \n",
    "   `who`, `adult_male`, `deck`, `embark_town`, `alive`, `alone`, `embarked`,  \n",
    "   plus engineered features such as `family_size`.\n",
    "\n",
    "3. **Which other features could improve predictions and why?**\n",
    "\n",
    "   - **Deck:** Could indicate luxury of accommodations (but contains many missing values).\n",
    "   - **Embarked:** Different ports may have different fare structures.\n",
    "   - **Embark_town:** Similar reasoning as embarked, related to departure location.\n",
    "   - **Class (string version of pclass):** Redundant but correlates with social/economic status.\n",
    "   \n",
    "   These features might add explanatory power because ticket prices were strongly tied to socioeconomic status, cabin location, and travel route.\n",
    "\n",
    "4. **How many variables are in Case 4?**\n",
    "\n",
    "   Case 4 includes **two variables:** `pclass` and `sex`.\n",
    "\n",
    "5. **Which variable(s) did you choose for Case 4 and why do you feel these could make good inputs?**\n",
    "\n",
    "   I chose **pclass** and **sex** for Case 4.  \n",
    "   - **Pclass** is directly tied to fare and represents one of the strongest predictors of ticket price.  \n",
    "   - **Sex** is included because passenger gender often correlates with cabin type, ticket grouping, and even travel class. These factors indirectly influence fare and may help improve model performance.  \n",
    "   \n",
    "   Together, these variables provide a stronger foundation for predicting fare compared to using demographic features like age or family size alone.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26750b0f",
   "metadata": {},
   "source": [
    "## 4. Linear Regression Models\n",
    "Train/test split, model fitting, predictions, and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a99c77d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {}\n",
    "\n",
    "for i, (X, y) in enumerate([(X1, y1), (X2, y2), (X3, y3), (X4, y4)], start=1):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "    splits[f\"X{i}_train\"], splits[f\"X{i}_test\"] = X_train, X_test\n",
    "    splits[f\"y{i}_train\"], splits[f\"y{i}_test\"] = y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15822f1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age            0\n",
       "family_size    0\n",
       "pclass         0\n",
       "sex            0\n",
       "fare           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdcc34ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic['sex'] = titanic['sex'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f88e331",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = titanic[['age', 'family_size', 'pclass', 'sex', 'fare']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d56403c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "preds = {}\n",
    "\n",
    "for i in range(1, 4 + 1):\n",
    "    X_train = splits[f\"X{i}_train\"]\n",
    "    y_train = splits[f\"y{i}_train\"]\n",
    "\n",
    "    model = LinearRegression().fit(X_train, y_train)\n",
    "    models[f\"lr{i}\"] = model\n",
    "\n",
    "    preds[f\"y{i}_pred_train\"] = model.predict(X_train)\n",
    "    preds[f\"y{i}_pred_test\"] = model.predict(splits[f\"X{i}_test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82e7861c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "\n",
    "def evaluate_case(i):\n",
    "    print(f\"----- Case {i} -----\")\n",
    "\n",
    "    # Training predictions\n",
    "    y_train = splits[f\"y{i}_train\"]\n",
    "    y_pred_train = preds[f\"y{i}_pred_train\"]\n",
    "\n",
    "    # Test predictions\n",
    "    y_test = splits[f\"y{i}_test\"]\n",
    "    y_pred_test = preds[f\"y{i}_pred_test\"]\n",
    "\n",
    "    # Training R2\n",
    "    print(\"Train R²:\", r2_score(y_train, y_pred_train))\n",
    "\n",
    "    # Test R2\n",
    "    print(\"Test R²:\", r2_score(y_test, y_pred_test))\n",
    "\n",
    "    # Test RMSE (manual computation)\n",
    "    rmse = mean_squared_error(y_test, y_pred_test) ** 0.5\n",
    "    print(\"Test RMSE:\", rmse)\n",
    "\n",
    "    # Test MAE\n",
    "    print(\"Test MAE:\", mean_absolute_error(y_test, y_pred_test))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "709a9e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Case 1 -----\n",
      "Train R²: 0.009950688019452314\n",
      "Test R²: 0.0034163395508415295\n",
      "Test RMSE: 37.97164180172938\n",
      "Test MAE: 25.28637293162364\n",
      "\n",
      "----- Case 2 -----\n",
      "Train R²: 0.049915792364760736\n",
      "Test R²: 0.022231186110131973\n",
      "Test RMSE: 37.6114940041967\n",
      "Test MAE: 25.02534815941641\n",
      "\n",
      "----- Case 3 -----\n",
      "Train R²: 0.07347466201590014\n",
      "Test R²: 0.049784832763073106\n",
      "Test RMSE: 37.0777586646559\n",
      "Test MAE: 24.284935030470688\n",
      "\n",
      "----- Case 4 -----\n",
      "Train R²: 0.30902741887346497\n",
      "Test R²: 0.339901132876393\n",
      "Test RMSE: 30.90345156449409\n",
      "Test MAE: 20.39966564200882\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 5):\n",
    "    evaluate_case(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7088b6",
   "metadata": {},
   "source": [
    "### Section 4 Reflection – Linear Regression Model Performance\n",
    "\n",
    "#### 1. Did Case 1 overfit or underfit? Explain.\n",
    "Case 1 used **age** as the only predictor. The Train R² was 0.0099 and the Test R² was 0.0034, which are extremely low. Because both the train and test scores were similarly poor, this case clearly **underfit**. Age alone does not meaningfully explain fare.\n",
    "\n",
    "#### 2. Did Case 2 overfit or underfit? Explain.\n",
    "Case 2 used **family_size** only. The Train R² (0.0499) and Test R² (0.0222) were slightly higher than Case 1 but still very low. The model performed poorly on both sets, which again indicates **underfitting**.\n",
    "\n",
    "#### 3. Did Case 3 overfit or underfit? Explain.\n",
    "Case 3 combined **age + family_size**. This did improve the performance a little (Train R² = 0.0737, Test R² = 0.0497), but the improvement was small and both values remained low. This model also **underfit**, meaning the features did not provide enough information to explain fare.\n",
    "\n",
    "#### 4. Did Case 4 overfit or underfit? Explain.\n",
    "Case 4 used **pclass + sex**, and this model performed much better than the first three. Train R² was 0.3090 and Test R² was 0.3399. The test score was even slightly higher, showing good generalization. This model **did not overfit** and used more meaningful predictors.\n",
    "\n",
    "---\n",
    "\n",
    "### Adding Age\n",
    "1. **Did adding age improve the model?**  \n",
    "   Adding age in Case 3 improved the R² slightly compared to Cases 1 and 2, but the improvement was small and the model was still weak.\n",
    "\n",
    "2. **Possible explanation:**  \n",
    "   Fare was not strongly tied to age. Ticket prices depended more on travel class and accommodations than whether the passenger was older or younger. This explains why age contributed little to predictive power.\n",
    "\n",
    "---\n",
    "\n",
    "### Worst Case\n",
    "1. **Which case performed the worst?**  \n",
    "   Case 1 (age only) performed the worst.\n",
    "\n",
    "2. **How do you know?**  \n",
    "   It had the **lowest R²** on both the training and test sets and the **highest RMSE and MAE**.\n",
    "\n",
    "3. **Would more training data help?**  \n",
    "   No. The issue is the **weak feature**, not the dataset size. Age simply does not explain fare well.\n",
    "\n",
    "---\n",
    "\n",
    "### Best Case\n",
    "1. **Which case performed the best?**  \n",
    "   Case 4 (pclass + sex) performed the best.\n",
    "\n",
    "2. **How do you know?**  \n",
    "   It had the **highest R²** and the **lowest RMSE and MAE** across all cases. These features captured meaningful information about fare.\n",
    "\n",
    "3. **Would more training data help?**  \n",
    "   Slightly, but the model is already using the strongest available predictors. Larger improvements would require adding additional relevant features, not more rows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b71601",
   "metadata": {},
   "source": [
    "# Section 5 - Alternative Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7183e641",
   "metadata": {},
   "source": [
    "#### 5.0 - Set up Best Case Data (Case 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43846d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_best = X4\n",
    "y_best = y4\n",
    "\n",
    "# Train/test split for best case\n",
    "X_train_best, X_test_best, y_train_best, y_test_best = train_test_split(\n",
    "    X_best, y_best, test_size=0.2, random_state=123\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c246be",
   "metadata": {},
   "source": [
    "#### 5.1 - Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095f5e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge_model = Ridge(alpha=1.0)\n",
    "ridge_model.fit(X_train_best, y_train_best)\n",
    "\n",
    "y_pred_ridge = ridge_model.predict(X_test_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332c24e1",
   "metadata": {},
   "source": [
    "#### 5.2 - Elastic Net Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "827a1515",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "elastic_model = ElasticNet(alpha=0.3, l1_ratio=0.5)\n",
    "elastic_model.fit(X_train_best, y_train_best)\n",
    "\n",
    "y_pred_elastic = elastic_model.predict(X_test_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a929f755",
   "metadata": {},
   "source": [
    "#### 5.3 — Polynomial Regression (degree = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5084f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "poly = PolynomialFeatures(degree=3)\n",
    "X_train_poly = poly.fit_transform(X_train_best)\n",
    "X_test_poly = poly.transform(X_test_best)\n",
    "\n",
    "poly_model = LinearRegression()\n",
    "poly_model.fit(X_train_poly, y_train_best)\n",
    "\n",
    "y_pred_poly = poly_model.predict(X_test_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297baf4c",
   "metadata": {},
   "source": [
    "#### 5.4 — Compare Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd2e96c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(name, y_true, y_pred):\n",
    "    rmse = mean_squared_error(y_true, y_pred) ** 0.5\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    print(f\"{name} R²:   {r2:.3f}\")\n",
    "    print(f\"{name} RMSE: {rmse:.2f}\")\n",
    "    print(f\"{name} MAE:  {mae:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "040c3e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge_model = Ridge(alpha=1.0)\n",
    "ridge_model.fit(X_train_best, y_train_best)\n",
    "\n",
    "y_pred_ridge = ridge_model.predict(X_test_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6bd479a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression R²:   0.340\n",
      "Linear Regression RMSE: 30.90\n",
      "Linear Regression MAE:  20.40\n",
      "\n",
      "Ridge R²:   0.340\n",
      "Ridge RMSE: 30.89\n",
      "Ridge MAE:  20.36\n",
      "\n",
      "Elastic Net R²:   0.369\n",
      "Elastic Net RMSE: 30.23\n",
      "Elastic Net MAE:  19.18\n",
      "\n",
      "Polynomial (deg 3) R²:   0.446\n",
      "Polynomial (deg 3) RMSE: 28.30\n",
      "Polynomial (deg 3) MAE:  17.61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression baseline (Case 4)\n",
    "y_pred_lr = models[\"lr4\"].predict(X_test_best)\n",
    "\n",
    "report(\"Linear Regression\", y_test_best, y_pred_lr)\n",
    "report(\"Ridge\", y_test_best, y_pred_ridge)\n",
    "report(\"Elastic Net\", y_test_best, y_pred_elastic)\n",
    "report(\"Polynomial (deg 3)\", y_test_best, y_pred_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394836af",
   "metadata": {},
   "source": [
    "### Section 5 Reflection – Comparing Alternative Regression Models\n",
    "\n",
    "#### 1. What patterns does the cubic polynomial model seem to capture?\n",
    "\n",
    "The polynomial model (degree 3) captures a more flexible and curved relationship between the predictors (pclass and sex) and fare. Because polynomial features include interaction terms and nonlinear transformations, the model is able to represent more complex price differences across passenger classes and between males and females. This helped the model fit patterns that a straight linear boundary could not capture.\n",
    "\n",
    "#### 2. Where does it perform well or poorly?\n",
    "\n",
    "The polynomial model performed well overall, producing the **highest R² (0.446)** and **lowest errors (RMSE ≈ 28.30, MAE ≈ 17.61)** of all models. It performs best in areas where the data is dense—such as common fare ranges—and where nonlinear effects exist. However, because the Titanic dataset for these features (just two columns) is simple, the model may overfit slightly by trying to force curvature where only limited variation exists.\n",
    "\n",
    "#### 3. Did the polynomial model outperform linear regression?\n",
    "\n",
    "Yes.  \n",
    "The baseline linear regression model (Case 4) produced an R² of **0.340**, while the polynomial model improved this to **0.446**. The errors also decreased significantly. This shows that adding non-linear interactions helped the model pick up additional relationships between class, sex, and fare.\n",
    "\n",
    "#### 4. Where does the polynomial fit best?\n",
    "\n",
    "The polynomial model fits best in the middle ranges of the fare values, where the majority of passengers are priced. It is also able to model differences between passenger classes more smoothly. However, because the input variables (pclass and sex) are low-dimensional and mostly categorical/binary, very high-order curves do not add meaningful complexity and may introduce noise at the extremes.\n",
    "\n",
    "---\n",
    "\n",
    "### Comparing All Models\n",
    "\n",
    "**Linear Regression (baseline):**  \n",
    "- Simple and interpretable  \n",
    "- R² = 0.340  \n",
    "- Good starting point, but limited flexibility  \n",
    "\n",
    "**Ridge Regression:**  \n",
    "- R² = 0.340 (same as Linear)  \n",
    "- Ridge does not help much because the model has few coefficients and no large weights to penalize  \n",
    "\n",
    "**Elastic Net:**  \n",
    "- R² = 0.369  \n",
    "- Slight improvement over simple linear regression  \n",
    "- Shows that mild regularization can help stabilize predictions  \n",
    "\n",
    "**Polynomial Regression (degree 3):**  \n",
    "- R² = 0.446  \n",
    "- Best performing model overall  \n",
    "- Able to model nonlinear relationships, but may risk mild overfitting if degree is too high  \n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "Overall, Polynomial Regression performed the best on the Titanic fare prediction task, while Ridge and Linear Regression produced nearly identical results. Elastic Net improved performance slightly. This indicates that a modest amount of nonlinearity helps capture patterns in how class and sex influence fare, but additional regularization or higher-order models should be used carefully to avoid overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7fc4a3d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>R²</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear (Case 4)</td>\n",
       "      <td>0.339901</td>\n",
       "      <td>30.903452</td>\n",
       "      <td>20.399666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.340454</td>\n",
       "      <td>30.890500</td>\n",
       "      <td>20.361302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Elastic Net</td>\n",
       "      <td>0.368549</td>\n",
       "      <td>30.225426</td>\n",
       "      <td>19.184114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Polynomial (deg 3)</td>\n",
       "      <td>0.446267</td>\n",
       "      <td>28.304320</td>\n",
       "      <td>17.614741</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Model        R²       RMSE        MAE\n",
       "0     Linear (Case 4)  0.339901  30.903452  20.399666\n",
       "1               Ridge  0.340454  30.890500  20.361302\n",
       "2         Elastic Net  0.368549  30.225426  19.184114\n",
       "3  Polynomial (deg 3)  0.446267  28.304320  17.614741"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = {\n",
    "    \"Model\": [\"Linear (Case 4)\", \"Ridge\", \"Elastic Net\", \"Polynomial (deg 3)\"],\n",
    "    \"R²\": [\n",
    "        r2_score(y_test_best, y_pred_lr),\n",
    "        r2_score(y_test_best, y_pred_ridge),\n",
    "        r2_score(y_test_best, y_pred_elastic),\n",
    "        r2_score(y_test_best, y_pred_poly),\n",
    "    ],\n",
    "    \"RMSE\": [\n",
    "        (mean_squared_error(y_test_best, y_pred_lr) ** 0.5),\n",
    "        (mean_squared_error(y_test_best, y_pred_ridge) ** 0.5),\n",
    "        (mean_squared_error(y_test_best, y_pred_elastic) ** 0.5),\n",
    "        (mean_squared_error(y_test_best, y_pred_poly) ** 0.5),\n",
    "    ],\n",
    "    \"MAE\": [\n",
    "        mean_absolute_error(y_test_best, y_pred_lr),\n",
    "        mean_absolute_error(y_test_best, y_pred_ridge),\n",
    "        mean_absolute_error(y_test_best, y_pred_elastic),\n",
    "        mean_absolute_error(y_test_best, y_pred_poly),\n",
    "    ],\n",
    "}\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bc7e18",
   "metadata": {},
   "source": [
    "## Section 6: Final Thoughts & Insights\n",
    "\n",
    "### 6.1 Summarize Findings\n",
    "\n",
    "This project explored multiple regression models to predict Titanic passenger fares using different feature combinations.  \n",
    "The early cases (1–3) performed poorly, with R² scores close to zero, showing that features such as **age** and **family_size** alone have very limited predictive power. The results highlight that these variables do not meaningfully explain fare variation.\n",
    "\n",
    "The best traditional model in Section 4 was **Case 4**, which used **pclass** and **sex**. These features showed a much stronger relationship with fare (Test R² ≈ 0.34), confirming that socioeconomic status (represented by passenger class) is the most influential factor in fare pricing.\n",
    "\n",
    "In Section 5, comparing alternative models revealed that:\n",
    "\n",
    "- **Linear Regression** and **Ridge Regression** performed similarly.  \n",
    "- **Elastic Net** offered a modest improvement.  \n",
    "- **Polynomial Regression (degree 3)** provided the best performance overall, raising R² to **~0.446** and reducing error significantly.\n",
    "\n",
    "This indicates that a modest amount of **nonlinearity** helps capture additional structure in the relationship between the predictors and fare.\n",
    "\n",
    "---\n",
    "\n",
    "### 6.2 Discuss Challenges\n",
    "\n",
    "Predicting fare turned out to be more challenging than expected. Although the Titanic dataset is well-known and commonly used, fare values are affected by a number of underlying factors that were not included in the simplified feature set used for this project. For example:\n",
    "\n",
    "- Fare distributions are **highly skewed**, with a few extremely expensive tickets.  \n",
    "- Some ticket types were bundled or shared among family members, making the individual fare less straightforward.  \n",
    "- Missing or inconsistent cabin and deck information limits the ability to capture the true effects of accommodations.  \n",
    "\n",
    "These factors contributed to modest predictive performance across models.  \n",
    "Even the best-performing model (Polynomial Regression) explained less than half of the variation in fare.\n",
    "\n",
    "---\n",
    "\n",
    "### 6.3 Optional Next Steps\n",
    "\n",
    "If time allowed, several improvements could further enhance model performance:\n",
    "\n",
    "1. **Include additional features** such as embarkation port, deck, ticket group size, or “alone/not alone” status.\n",
    "2. **Try predicting age instead of fare**, a target that may yield clearer linear relationships.\n",
    "3. **Apply log transformation to fare** to reduce skew and stabilize variance, which could improve linear regression performance.\n",
    "4. **Use more advanced models**, such as Random Forest or Gradient Boosting, which often perform better on structured/tabular data.\n",
    "5. **Investigate feature interactions systematically**, especially those involving passenger class and travel group structure.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "Overall, this project demonstrated the importance of thoughtful feature selection, the usefulness of evaluating multiple regression approaches, and the value of incorporating nonlinear modeling techniques. Passenger class and sex emerged as the strongest predictors of fare, and Polynomial Regression achieved the best performance by allowing the model to learn more complex patterns within those features.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "applied-ml-dstcyr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
