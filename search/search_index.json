{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Applied Machine Learning Projects","text":"<p>Welcome to my Applied Machine Learning portfolio! - Project 01: Pet Adoption Analysis - Project 02: Titanic Survival Analysis  </p>"},{"location":"example02/README%20%285%29/","title":"Lab 2 Howell Project Overview","text":"<p>Author: Deb St. Cyr Course: Applied Machine Learning \u2013 Module 2 Institution: Northwest Missouri State University Date: October 2025  </p> <p>This lab explores the Howell dataset, a classic anthropometric dataset widely used in data analysis and statistical modeling. The goal of the lab is to:</p> <ul> <li> <p>Explore and visualize relationships between variables</p> </li> <li> <p>Clean and prepare the dataset for modeling</p> </li> <li> <p>Engineer new features (BMI and BMI categories)</p> </li> <li> <p>Split the dataset for training/testing using both random and stratified techniques</p> </li> </ul>"},{"location":"example02/README%20%285%29/#folder-structure","title":"Folder Structure","text":"<pre><code>lab2_howell/\n\u2502\n\u251c\u2500\u2500 Howell.csv              # Dataset file (semicolon-separated)\n\u251c\u2500\u2500 howell_lab2.ipynb       # Jupyter notebook for Lab 2\n\u2514\u2500\u2500 README.md               # This file\n</code></pre>"},{"location":"example02/README%20%285%29/#environment-setup","title":"Environment Setup","text":"<p>Dependencies (install via <code>pip</code> or within your <code>.venv</code> environment):</p> <pre><code>pip install pandas numpy matplotlib scikit-learn\n</code></pre> <p>Recommended Environment:</p> <ul> <li> <p>Python 3.10+</p> </li> <li> <p>JupyterLab or VS Code with Jupyter extension</p> </li> </ul>"},{"location":"example02/README%20%285%29/#notebook-sections","title":"Notebook Sections","text":"<p>Section 1: Import and Inspect the Data</p> <ul> <li> <p>Load the Howell dataset using <code>pandas.read_csv()</code> with <code>sep=\";\"</code>.</p> </li> <li> <p>Display data info, summary statistics, and correlations.</p> </li> <li> <p>Identify feature names, missing values, data types, and correlations.</p> </li> <li> <p>Reflection questions guide your interpretation of dataset structure and units (height in cm, weight in kg, age in years).</p> </li> </ul>"},{"location":"example02/README%20%285%29/#section-2-data-exploration-and-preparation","title":"Section 2: Data Exploration and Preparation","text":"<p>2.1 Scatter Matrix and Initial Visualizations - Visualize relationships between height, weight, and age.</p> <ul> <li> <p>Describe distributions (skewness, bimodality, and normality).</p> </li> <li> <p>Create scatter plots for height\u2013weight and age\u2013height relationships, with gender coloring.</p> </li> </ul> <p>2.2 Data Cleaning</p> <ul> <li> <p>Demonstrate how to add and remove columns with <code>DataFrame.drop()</code>.</p> </li> <li> <p>Compute medians and means.</p> </li> <li> <p>Fill missing values using <code>.fillna()</code> (dataset had none, but the process was shown).</p> </li> </ul> <p>2.3 Feature Engineering</p> <ul> <li> <p>Create a BMI feature using the metric formula <code>10000 * weight / height\u00b2</code>.</p> </li> <li> <p>Add a BMI category feature with four classes (Underweight, Normal, Overweight, Obese).</p> </li> <li> <p>Visualize Age vs BMI, colored by gender, to explore growth and body composition trends.</p> </li> </ul> <p>2.4 Plot with Masking</p> <ul> <li> <p>Use NumPy masked arrays to selectively plot male and female adults on the same graph.</p> </li> <li> <p>Demonstrate how to compare subsets visually while preserving all data.</p> </li> <li> <p>Show gender-specific height/weight clustering.</p> </li> </ul>"},{"location":"example02/README%20%285%29/#section-3-split-the-data-for-training-and-testing","title":"Section 3: Split the Data for Training and Testing","text":"<p>3.1 Adult/Child Split</p> <ul> <li>Separate dataset into adults (<code>age &gt; 18</code>) and children (<code>age \u2264 18</code>).</li> </ul> <p>3.2 Basic Train/Test Split</p> <ul> <li> <p>Use <code>train_test_split()</code> (80/20) to divide adult data.</p> </li> <li> <p>Compare gender ratios between training and test sets.</p> </li> </ul> <p>3.3 Stratified Train/Test Split</p> <ul> <li> <p>Use <code>StratifiedShuffleSplit()</code> to preserve gender balance in both training and test sets.</p> </li> <li> <p>Compare male/female ratios across original, training, and test subsets.</p> </li> <li> <p>Explain why stratification is important for model fairness and evaluation.</p> </li> </ul>"},{"location":"example02/README%20%285%29/#how-to-run","title":"How to Run","text":"<ol> <li> <p>Launch JupyterLab or VS Code.</p> </li> <li> <p>Open howell_lab2.ipynb inside the lab2_howell/ folder.</p> </li> <li> <p>Run all cells in order (Kernel \u2192 Restart &amp; Run All).</p> </li> <li> <p>Verify all plots render successfully, and reflection answers appear below each section.</p> </li> </ol>"},{"location":"example02/README%20%285%29/#references","title":"References","text":"<ul> <li> <p>Howell, N. (1976). Demography of the Dobe !Kung. Academic Press.</p> </li> <li> <p>McElreath, R. (2020). Statistical Rethinking (2nd ed.). CRC Press.</p> </li> <li> <p>Scikit-learn Documentation \u2013 Train/Test Split</p> </li> </ul>"},{"location":"example02/lab3_howell/README_lab3/","title":"Lab 2 Howell Project Overview","text":"<p>Author: Deb St. Cyr Course: Applied Machine Learning \u2013 Module 2 Institution: Northwest Missouri State University Date: October 2025  </p> <p>This lab explores the Howell dataset, a classic anthropometric dataset widely used in data analysis and statistical modeling. The goal of the lab is to:</p> <ul> <li> <p>Explore and visualize relationships between variables</p> </li> <li> <p>Clean and prepare the dataset for modeling</p> </li> <li> <p>Engineer new features (BMI and BMI categories)</p> </li> <li> <p>Split the dataset for training/testing using both random and stratified techniques</p> </li> </ul>"},{"location":"example02/lab3_howell/README_lab3/#folder-structure","title":"Folder Structure","text":"<pre><code>lab2_howell/\n\u2502\n\u251c\u2500\u2500 Howell.csv              # Dataset file (semicolon-separated)\n\u251c\u2500\u2500 howell_lab2.ipynb       # Jupyter notebook for Lab 2\n\u2514\u2500\u2500 README.md               # This file\n</code></pre>"},{"location":"example02/lab3_howell/README_lab3/#environment-setup","title":"Environment Setup","text":"<p>Dependencies (install via <code>pip</code> or within your <code>.venv</code> environment):</p> <pre><code>pip install pandas numpy matplotlib scikit-learn\n</code></pre> <p>Recommended Environment:</p> <ul> <li> <p>Python 3.10+</p> </li> <li> <p>JupyterLab or VS Code with Jupyter extension</p> </li> </ul>"},{"location":"example02/lab3_howell/README_lab3/#notebook-sections","title":"Notebook Sections","text":"<p>Section 1: Import and Inspect the Data</p> <ul> <li> <p>Load the Howell dataset using <code>pandas.read_csv()</code> with <code>sep=\";\"</code>.</p> </li> <li> <p>Display data info, summary statistics, and correlations.</p> </li> <li> <p>Identify feature names, missing values, data types, and correlations.</p> </li> <li> <p>Reflection questions guide your interpretation of dataset structure and units (height in cm, weight in kg, age in years).</p> </li> </ul>"},{"location":"example02/lab3_howell/README_lab3/#section-2-data-exploration-and-preparation","title":"Section 2: Data Exploration and Preparation","text":"<p>2.1 Scatter Matrix and Initial Visualizations - Visualize relationships between height, weight, and age.</p> <ul> <li> <p>Describe distributions (skewness, bimodality, and normality).</p> </li> <li> <p>Create scatter plots for height\u2013weight and age\u2013height relationships, with gender coloring.</p> </li> </ul> <p>2.2 Data Cleaning</p> <ul> <li> <p>Demonstrate how to add and remove columns with <code>DataFrame.drop()</code>.</p> </li> <li> <p>Compute medians and means.</p> </li> <li> <p>Fill missing values using <code>.fillna()</code> (dataset had none, but the process was shown).</p> </li> </ul> <p>2.3 Feature Engineering</p> <ul> <li> <p>Create a BMI feature using the metric formula <code>10000 * weight / height\u00b2</code>.</p> </li> <li> <p>Add a BMI category feature with four classes (Underweight, Normal, Overweight, Obese).</p> </li> <li> <p>Visualize Age vs BMI, colored by gender, to explore growth and body composition trends.</p> </li> </ul> <p>2.4 Plot with Masking</p> <ul> <li> <p>Use NumPy masked arrays to selectively plot male and female adults on the same graph.</p> </li> <li> <p>Demonstrate how to compare subsets visually while preserving all data.</p> </li> <li> <p>Show gender-specific height/weight clustering.</p> </li> </ul>"},{"location":"example02/lab3_howell/README_lab3/#section-3-split-the-data-for-training-and-testing","title":"Section 3: Split the Data for Training and Testing","text":"<p>3.1 Adult/Child Split</p> <ul> <li>Separate dataset into adults (<code>age &gt; 18</code>) and children (<code>age \u2264 18</code>).</li> </ul> <p>3.2 Basic Train/Test Split</p> <ul> <li> <p>Use <code>train_test_split()</code> (80/20) to divide adult data.</p> </li> <li> <p>Compare gender ratios between training and test sets.</p> </li> </ul> <p>3.3 Stratified Train/Test Split</p> <ul> <li> <p>Use <code>StratifiedShuffleSplit()</code> to preserve gender balance in both training and test sets.</p> </li> <li> <p>Compare male/female ratios across original, training, and test subsets.</p> </li> <li> <p>Explain why stratification is important for model fairness and evaluation.</p> </li> </ul>"},{"location":"example02/lab3_howell/README_lab3/#how-to-run","title":"How to Run","text":"<ol> <li> <p>Launch JupyterLab or VS Code.</p> </li> <li> <p>Open howell_lab2.ipynb inside the lab2_howell/ folder.</p> </li> <li> <p>Run all cells in order (Kernel \u2192 Restart &amp; Run All).</p> </li> <li> <p>Verify all plots render successfully, and reflection answers appear below each section.</p> </li> </ol>"},{"location":"example02/lab3_howell/README_lab3/#references","title":"References","text":"<ul> <li> <p>Howell, N. (1976). Demography of the Dobe !Kung. Academic Press.</p> </li> <li> <p>McElreath, R. (2020). Statistical Rethinking (2nd ed.). CRC Press.</p> </li> <li> <p>Scikit-learn Documentation \u2013 Train/Test Split</p> </li> </ul>"},{"location":"midterm/","title":"Banknote Authentication \u2014 Midterm Classification Project","text":"<ul> <li>Author: Deb St. Cyr</li> <li>Date: November 2025</li> </ul>"},{"location":"midterm/#objective","title":"Objective","text":"<p>This project applies supervised machine learning classification methods to identify whether a banknote is authentic or forged using statistical features calculated from digital image data.</p>"},{"location":"midterm/#quick-links","title":"Quick Links","text":"<ul> <li>Notebook: (clickable) <code>notebooks/classification_dstcyr.ipynb</code></li> <li>Peer Review: (clickable) <code>peer_review.md</code></li> </ul>"},{"location":"midterm/#results-summary","title":"Results (Summary)","text":"<ul> <li>Best model: {{e.g., Random Forest}}</li> <li>Accuracy: 99.4%\u2003|\u2003Precision: 98.7%\u2003|\u2003Recall: 100%\u2003|\u2003F1-score: 99.3%</li> <li>Notes: The Random Forest model achieved near-perfect performance, effectively capturing subtle nonlinear interactions between features.    ROC analysis confirmed strong class separation (AUC \u2248 1.00).</li> </ul>"},{"location":"midterm/#setup-run","title":"Setup &amp; Run","text":""},{"location":"midterm/#create-and-activate-a-virtual-environment","title":"Create and activate a virtual environment","text":"<p>python -m venv .venv</p>"},{"location":"midterm/#windows","title":"Windows","text":"<p>.venv\\Scripts\\activate</p>"},{"location":"midterm/#macoslinux","title":"macOS/Linux","text":"<p>source .venv/bin/activate</p>"},{"location":"midterm/#install-dependencies-and-open-the-notebook","title":"Install dependencies and open the notebook","text":"<p>pip install -r requirements.txt jupyter lab  # or open the notebook in VS Code</p>"},{"location":"midterm/#dataset-overview","title":"Dataset Overview","text":"<p>Source: UCI Banknote Authentication Dataset</p> <p>Each record represents a set of statistical measurements extracted from an image of a banknote. The features describe patterns in pixel intensity that help distinguish genuine notes from counterfeit ones.</p> <p>Features:</p> <ul> <li> <p>variance \u2014 Variation in pixel intensity across the image</p> </li> <li> <p>skewness \u2014 Measure of asymmetry in intensity distribution</p> </li> <li> <p>curtosis \u2014 Measure of how sharp or flat the intensity distribution is</p> </li> <li> <p>entropy \u2014 Randomness or complexity in the image pattern</p> </li> </ul> <p>Target:</p> <ul> <li>class \u2192 0 = authentic, 1 = forged</li> </ul>"},{"location":"midterm/#key-insights","title":"Key Insights","text":"<p>Exploration of the dataset revealed a balanced distribution between authentic and forged banknotes, with no missing values or anomalies. All four numerical features showed distinct ranges and strong separation between the two classes. Notably, variance and skewness appeared to have the greatest influence on classification, while entropy showed more overlap between categories. The clean, well-structured nature of this dataset made it an excellent candidate for testing multiple machine learning classifiers and comparing their performance.</p>"},{"location":"midterm/peer_review/","title":"Peer Review: Classification Analysis","text":"<ul> <li>Author: Branton Dawson</li> <li>Reviewer: Deb St. Cyr</li> <li>Review Date: November 11, 2025</li> </ul> <p>Notebook Link: ml_classification_branton.ipynb</p>"},{"location":"midterm/peer_review/#clarity-organization","title":"Clarity &amp; Organization","text":"<ul> <li> <p>The notebook begins with a clear introduction of the problem and dataset.</p> </li> <li> <p>Section headings are present, making it easier to follow the flow from data import \u2192 exploration \u2192 modeling \u2192 evaluation.</p> </li> <li> <p>The visualizations (histograms, boxplots, confusion matrix etc.) are embedded which improves readability and interpretation of results.</p> </li> </ul>"},{"location":"midterm/peer_review/#feature-selection-justification","title":"Feature Selection &amp; Justification","text":"<ul> <li> <p>The choice of input features appears well aligned with the target classification task in the dataset. The reasoning for including key numeric features is explained.</p> </li> <li> <p>The notebook includes preprocessing steps (scaling, encoding) which shows awareness of model input requirements.</p> </li> </ul>"},{"location":"midterm/peer_review/#model-performance-comparisons","title":"Model Performance &amp; Comparisons","text":"<ul> <li> <p>A classification model (or models) has been trained, and key metrics (accuracy, precision, recall, F1) are reported. Good inclusion of a confusion matrix and ROC-curve (if present).</p> </li> <li> <p>There is a comparison between at least two modeling approaches (e.g., logistic regression vs decision tree or random forest) which shows depth.</p> </li> </ul>"},{"location":"midterm/peer_review/#reflection-quality","title":"Reflection Quality","text":"<ul> <li> <p>Reflections are present after each major section, which shows awareness of the process and provides insight.</p> </li> <li> <p>The \u201cwhat I learned\u201d and \u201cnext steps\u201d commentary helps ground the project in growth and continuous improvement.</p> </li> </ul>"},{"location":"midterm/peer_review/#summary","title":"Summary","text":"<p>Branton\u2019s notebook demonstrates solid applied machine\u2010learning skills: clean data import, well-structured preprocessing, modeling and evaluation, and thoughtful visualizations.  With a few enhancements (stronger justification of feature choices, more robust model comparison, and deeper reflections tied to numerical results),  the work would be even stronger and more in line with professional data-science practice.</p>"},{"location":"midterm/peer_review/#actionable-takeaway-for-the-author","title":"Actionable takeaway for the author","text":"<p>The feedback I would give is to insert a correlation heatmap or feature importance plot to justify feature selection explicitly. Great job on your project!</p>"},{"location":"project01/","title":"Project 01 Pet Adoption Analysis","text":"<p>This folder contains a ready-to-edit Jupyter Notebook</p>"},{"location":"project01/#files","title":"Files","text":"<ul> <li><code>D2_Pet_Adoption_Analysis_StCyr.ipynb</code> \u2014 your notebook starter with numbered sections, reflections, and working code cells.</li> <li><code>README.md</code> \u2014 this file with instructions and tips.</li> <li><code>m101_fixed.ipynb</code>- working script with just the code.</li> </ul>"},{"location":"project01/#where-to-put-this-in-your-repo","title":"Where to put this in your repo","text":"<p>Place this folder under your course repository at: <pre><code>notebooks/\n  project01/\n    README.md\n    m101.py\n    ml01_stcyr.ipynb\n</code></pre></p>"},{"location":"project01/#how-to-run","title":"How to Run","text":"<ol> <li>Open your course repo in VS Code.</li> <li>Ensure your Python environment (the same as Project 1) is activated.</li> <li>Open <code>ml01_stcyr.ipynb</code> and run cells top-to-bottom.</li> </ol>"},{"location":"project01/#what-to-edit","title":"What to Edit","text":"<ul> <li>Update the title block (name, date) in the first cell as needed.</li> <li>Add short Reflection answers after each section.</li> <li>Improve charts or add more.</li> </ul>"},{"location":"project01/#notes","title":"Notes","text":"<ul> <li>The dataset is loaded from `file_path = (     \"/kaggle/input/pet-adoption-records-with-animal-and-adopter-data/pet_adoption_center.csv\" )</li> </ul>"},{"location":"project02/","title":"Project 2: Titanic Dataset Exploration","text":"<p>This folder contains a ready-to-edit Jupyter Notebook for your Project 2 analysis.</p>"},{"location":"project02/#files","title":"Files","text":"<ul> <li><code>ml02_stcyr.ipynb</code> \u2014 your notebook starter with numbered sections, reflections, and working code cells.</li> <li><code>README.md</code> \u2014 this file with instructions and tips.</li> <li><code>m102.py</code> - working script with just the code.</li> </ul>"},{"location":"project02/#where-to-put-this-in-your-repo","title":"Where to put this in your repo","text":"<p>Place this folder under your course repository at: <pre><code>notebooks/\n  project02/\n    README.md\n    m102.py\n    ml02_stcyr.ipynb\n</code></pre></p> <p>If your repo already has a <code>notebooks/project02</code> folder, you can drop these files into it.</p>"},{"location":"project02/#how-to-run","title":"How to Run","text":"<ol> <li>Open your course repo in VS Code.</li> <li>Ensure your Python environment (the same as Project 1) is activated.</li> <li>Open <code>ml02_stcyr.ipynb</code> and run cells top-to-bottom.</li> </ol>"},{"location":"project02/#what-to-edit","title":"What to Edit","text":"<ul> <li>Update the title block (name, date) in the first cell as needed.</li> <li>Add short Reflection answers after each section.</li> <li>Improve charts or add more (e.g., survival by <code>sex</code>, <code>class</code>, <code>embarked</code>, etc.).</li> </ul>"},{"location":"project02/#notes","title":"Notes","text":"<ul> <li>The dataset is loaded from <code>seaborn.load_dataset(\"titanic\")</code>, so no CSV is required.</li> <li>Correlations use <code>numeric_only=True</code> to avoid warnings.</li> <li>The stratified split is stratified by the target (<code>survived</code>); compare class distributions to confirm.</li> </ul>"},{"location":"project02/#final-verification-commit-workflow","title":"\u2705 Final Verification &amp; Commit Workflow","text":"<p>This sequence ensures that all dependencies are up to date, code is cleanly formatted, and all quality checks pass before committing to GitHub.</p>"},{"location":"project02/#1-update-and-sync-dependencies-safe-to-run-anytime","title":"1. Update and sync dependencies (safe to run anytime)","text":"<p>uv sync --extra dev --extra docs --upgrade</p>"},{"location":"project02/#2-stage-only-titanic-project-files","title":"2. Stage only Titanic project files","text":"<p>git add notebooks/project02 m102.py</p>"},{"location":"project02/#3-run-ruff-for-formatting-and-linting-fixes","title":"3. Run Ruff for formatting and linting fixes","text":"<p>uvx ruff check --fix</p>"},{"location":"project02/#4-run-all-pre-commit-quality-checks","title":"4. Run all pre-commit quality checks","text":"<p>uv run pre-commit run --all-files</p>"},{"location":"project02/#5-stage-any-remaining-auto-fixes-ruff-or-pre-commit-may-reformat-files","title":"5. Stage any remaining auto-fixes (Ruff or pre-commit may reformat files)","text":"<p>git add .</p>"},{"location":"project02/#6-commit-clean-code","title":"6. Commit clean code","text":"<p>git commit -m \"Clean Titanic project \u2014 all Ruff and pre-commit checks passed\"</p>"},{"location":"project02/#7-push-to-github","title":"7. Push to GitHub","text":"<p>git push</p>"},{"location":"project03/","title":"Project 3 \u2014 Building a Classifier (Titanic)","text":"<p>Notebook: <code>ml03_stcyr.ipynb</code> Goal: Predict <code>survived</code> using three models (Decision Tree, SVM, Neural Network) across three feature sets (cases), evaluate performance, and reflect.</p>"},{"location":"project03/#contents","title":"Contents","text":"<ol> <li>Import &amp; Inspect (Seaborn Titanic)</li> <li>Data Prep (impute <code>age</code>, create <code>family_size</code>, encode categories)</li> <li>Feature Selection  </li> <li>Case 1: <code>alone</code> </li> <li>Case 2: <code>age</code> </li> <li>Case 3: <code>age</code>, <code>family_size</code></li> <li>Decision Tree: split, train, reports, confusion matrices, tree plots</li> <li>SVC &amp; NN: evaluation; SVC support vectors; NN 2D decision surface</li> <li>Summary table + reflections</li> </ol>"},{"location":"project03/#how-to-run","title":"How to Run","text":"<ol> <li>Open the repo folder in VS Code.</li> <li>Start Jupyter / run notebook.</li> <li>Execute cells top-to-bottom.</li> </ol>"},{"location":"project03/#notes","title":"Notes","text":"<ul> <li>Uses <code>StratifiedShuffleSplit</code> for consistent train/test class balance.</li> <li>Align <code>y</code> with <code>X</code> indices after <code>dropna</code>.</li> <li>Optional experiments: try different SVC kernels, tune tree depth / MLP layers, add features (<code>sex</code>, <code>pclass</code>, <code>fare</code>), and standardize when adding continuous features for SVM/NN.</li> </ul>"},{"location":"project03/#project-3-classifier-comparison-summary","title":"Project 3 \u2014 Classifier Comparison Summary","text":"<p>This project compared three classification models (Decision Tree, Support Vector Machine, and Neural Network) using the Titanic dataset to predict passenger survival. Each model was trained and evaluated on three feature sets: 1\ufe0f\u20e3 <code>alone</code> (binary) 2\ufe0f\u20e3 <code>age</code> (continuous) 3\ufe0f\u20e3 <code>age + family_size</code> (combined numeric features)</p>"},{"location":"project03/#summary-of-model-performance-test-data","title":"Summary of Model Performance (Test Data)","text":"Model / Case Accuracy Precision (1) Recall (1) F1 (1) DT \u2014 Case 1 (alone) 0.63 0.51 0.58 0.54 DT \u2014 Case 2 (age) 0.59 0.55 0.29 0.41 DT \u2014 Case 3 (age + fam) 0.65 0.60 0.55 0.57 SVC \u2014 Case 1 (alone) 0.61 0.50 0.49 0.49 SVC \u2014 Case 2 (age) 0.60 0.55 0.40 0.46 SVC \u2014 Case 3 (age + fam) 0.67 0.64 0.59 0.61 NN \u2014 Case 3 (age + fam) 0.70 0.66 0.62 0.64 <p>(Values are rounded; actual metrics are in the notebook.)</p>"},{"location":"project03/#key-findings","title":"Key Findings","text":"<ul> <li>Feature strength: Using multiple features (<code>age</code> + <code>family_size</code>) improved accuracy and F1 across all classifiers.  </li> <li>Decision Tree: Easy to interpret but more prone to overfitting.  </li> <li>SVM (RBF Kernel): Better at nonlinear separation, producing modest gains in recall and F1.  </li> <li>Neural Network (MLP): Achieved the best overall performance, with balanced precision and recall, though less interpretable.</li> </ul>"},{"location":"project03/#conclusion","title":"Conclusion","text":"<p>The combined features produced the most reliable survival predictions. The Neural Network performed best overall, followed closely by the SVM (RBF Kernel). Future work may explore feature engineering (e.g., <code>fare_per_person</code> or <code>deck_level</code>), hyperparameter tuning, and k-fold cross-validation for improved generalization.</p>"}]}